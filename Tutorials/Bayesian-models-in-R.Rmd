---
title: 'Tutorial: Bayesian models in R'
author: "Joshua Cook"
date: "10/24/2019"
output:
    html_document:
        theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(rethinking)
library(readxl)
library(tidyverse)
```

I stumbled across this tutorial on [*Bayesian modeling in R*](https://www.r-bloggers.com/bayesian-models-in-r-2/) and began reading through it.
It was quite thorough on both theory and application, so I figured it would be a good one to follow more closely.

## Theory

### Maximum liklihood estimation

Say we are at a casino and playing roulette.
We can bet on the color being either red ($r$) or black ($B$) with seemingly equal probability.
The last 10 draws were 8 black and 2 reds:

$B, B, r, B, B, B, r, B, B, B$

Using the frequentist approach, we would assigned the probability of getting black as $f(B) = \frac{8}{10}$.
This is the maiximum liklihood estimate, and would be a reasonable choice from a frequentist point-of-view.
However, we can count the number of black and red squares and believe that the ball has a random chance of landing in any of them.
Therefore, we believe that the probability of getting black should be $P(B) = 0.5$.

Alternatively, we can estimate the liklihood of getting the observed data (8 of 10 as black) over many different probabilities for getting black.
In the plot below, the x-axis values are 100 different probabilities (between 0 and 1) of black ($P(B)$) and the y-axis is the liklihood of getting 8 blacks from 10 rounds (ie. 8 successes from 10 trials).

```{r}
# 100 values for P(B).
range_p <- seq(0, 1, length.out = 100)
# Liklihood of 8/10 given all trials probabilities.
densities <- dbinom(x = 8, prob = range_p, size = 10)
```

```{r, echo=FALSE}
tibble(x = range_p, y = densities) %>% 
    ggplot(aes(x = x)) +
    geom_line(aes(y = y), color = "grey25") +
    scale_y_continuous(expand = expand_scale(mult = c(0, 0.02))) +
    scale_x_continuous(expand = c(0, 0)) +
    theme_bw() +
    labs(
        title = "Liklihood at different probabilites of getting black",
        x = "P(B)",
        y = "density"
    )
```

Our previous prediction for the probabiltiy of getting black was P(0.8) because it was the probability with the highest liklihood.
This is why it was called the Maximum Liklihood Estimate (MLE).

### Update beliefs: the prior distribution

Though we observed 8 of 10 trials coming up black, we still have a feeling that the probability should be 0.5 because there are equal numbers of black and red slots where each one has equal probability of being selected.
This is called the *prior* and it too has a distribution like that above (shown in red in the following plot).
(For the plot, the values for the prior are scaled down for visualization purposes, though it does not effect the mathematics.)

```{r}
prior <- dnorm(x = range_p, mean = 0.5, sd = .1)
```

```{r, echo=FALSE}
tibble(x = range_p, mle_y = densities, prior_y = prior / 15) %>% 
    ggplot(aes(x = x)) +
    geom_line(aes(y = mle_y), color = "grey25") +
    geom_line(aes(y = prior_y), color = "red") +
    scale_y_continuous(expand = expand_scale(mult = c(0, 0.02))) +
    scale_x_continuous(expand = c(0, 0)) +
    theme_bw() +
    labs(
        title = "Liklihood at different probabilites of getting black",
        x = "P(B)",
        y = "density"
    )
```

We can calculate the numerator for Bayes' theorem by multiplying the liklihood of acheiving 8 of 10 black for each $P(B)$ by the prior distribution (shown in green below).

```{r}
lik <- dbinom(x = 8, prob = range_p, size = 10)
prior <- dnorm(x = range_p, mean = 0.5, sd = 0.1)
bt_numerator <- lik * prior
```

```{r, echo=FALSE}
tibble(x = range_p, mle_y = densities, prior_y = prior / 15, num = bt_numerator) %>% 
    ggplot(aes(x = x)) +
    geom_line(aes(y = mle_y), color = "grey25") +
    geom_line(aes(y = prior_y), color = "red") +
    geom_line(aes(y = num), color = "forestgreen") +
    scale_y_continuous(expand = expand_scale(mult = c(0, 0.02))) +
    scale_x_continuous(expand = c(0, 0)) +
    theme_bw() +
    labs(
        title = "Liklihood at different probabilites of getting black",
        x = "P(B)",
        y = "density"
    )
```

### Standardizing the posterior

The area under the curve of the probability mass function must sum to 1.
In BT, this is solved by the denominator.

```{r}
bt_posterior <- bt_numerator / sum(bt_numerator)
```

```{r, echo=FALSE}
tibble(x = range_p, mle_y = densities, prior_y = prior / 15, num = bt_numerator, post = bt_posterior) %>% 
    ggplot(aes(x = x)) +
    geom_line(aes(y = mle_y), color = "grey25") +
    geom_line(aes(y = prior_y), color = "red") +
    geom_line(aes(y = num), color = "forestgreen") +
    geom_line(aes(y = post), color = "blue") +
    scale_y_continuous(expand = expand_scale(mult = c(0, 0.02))) +
    scale_x_continuous(expand = c(0, 0)) +
    theme_bw() +
    labs(
        title = "Liklihood at different probabilites of getting black",
        x = "P(B)",
        y = "density"
    )
```

## Simulation

Here is another example, however this time, we will be estimating two paramters, $\mu$ and $\sigma$, from a normal distribution.

For the purpose of the simulation, we will set $\mu=5$ and $\sigma=2$, but the simulation will not know this.
Here is the underlying model and our two priors.

$$
\text{model:} \quad X \sim N(\mu, \sigma) \\
\text{priors:} \quad \mu \sim N(0,5) \quad \sigma \sim Exp(1)
$$

Now we need the following components:

1. the grid of $\mu$ and $\sigma$ over which to search, we will use 200 values of each
2. compute the liklihood for each cell in the 200 x 200 grid
3. compute the product between the liklihoods and our priors
4. standardize the numerator


We begin by declaring the real, underlying values from which the data is sampled.

```{r}
# Declare the underlying values.
true_mu <- 5
true_sigma <- 2

# Sample from the distribution.
set.seed(0)
data_sample <- rnorm(100, true_mu, true_sigma)
```

The we create the grid for the search.
We decided to try 200 values for each parameter, with various ranges that could be decided from the range of the sampled data.

```{r}
grid <- expand.grid(
    mu = seq(0, 10, length.out = 200),
    sigma = seq(1, 3, length.out = 200)
)
```

We now compute the liklihood for each cell, a value for $\mu$ and $\sigma$.
Normaly, the liklihood of finding the data for a given $\mu$ and $\sigma$ would be found by taking the product of the probabilities: $Lik_i = \prod_{j=1}^{n} P(\mu_i, \sigma_i | X_j)$.
However, a more computationally efficient mechanism is to take the sum in the log-scale: $Lik_i = \sum_{j=1}^{n} log(P(\mu_i, \sigma_i | X_j))$.

```{r}
# Calculate the liklihood of seeing `data` given a mean of `mu` and a
#   std. dev. of `sigma`.
calculate_liklihood <- function(data, mu, sigma) {
    sum(dnorm(x = data, mean = mu, sd = sigma, log = TRUE))
}

lik <- purrr::pmap_dbl(grid, calculate_liklihood, data = data_sample)
```

Then we can multiply the liklihoods with the priors.
Since we are in log-scale, this means we add the priors.

```{r}
bt_numerator <- lik + dnorm(grid$mu, mean = 0, sd = 5, log = TRUE) + dexp(grid$sigma, 1, log = TRUE)
```

Finally, we need to standardize the numerator with the demonintator to bring the integral over the PMF to 1.
We must do this in the exponent because we are in log-scale.

```{r}
bt_posterior <- exp(bt_numerator - max(bt_numerator))
```

Now we can plot the known and estimated values for $\mu$ and $\sigma$ on a sample taken from the posterior distribution.


```{r}
post_estimates <- grid[which.max(bt_posterior), ]
post_sample <- sample_n(grid, size = 1e3, weight = bt_posterior, replace = TRUE)
```

The posterior estimates are:

- $\mu$: `r round(post_estimates$mu, 3)`
- $\sigma$: `r round(post_estimates$sigma, 3)`


```{r, echo=FALSE}
as_tibble(post_sample) %>%
    ggplot(aes(x = mu, y = sigma)) +
    geom_jitter(alpha = 0.2) +
    geom_hline(yintercept = true_sigma, color = "red", linetype = 1) +
    geom_vline(xintercept = true_mu, color = "red", linetype = 1) +
    geom_hline(yintercept = post_estimates$sigma, color = "purple", linetype = 1) +
    geom_vline(xintercept = post_estimates$mu, color = "purple", linetype = 1) +
    theme_bw() +
    labs(
        title = "Posterior distribution",
        x = "estimated mean",
        y = "estimated std. dev."
    )
```

## Bayesian models and MCMC

The author recommends two packages for Bayesian modeling, ['greta'](https://cran.r-project.org/web/packages/greta/index.html) and ['rethinking'](https://github.com/rmcelreath/rethinking) (not on CRAN), and compares their features.

## Bayesian modeling in R

### Data

For this example, we used data collected for a study on the reproductive behaviour in *Crotophaga major*, a type of cuckoo.
The females display both coopertative and parasitic nesting behaviour.
There are three hypothesis we have for this study:

1. *super monther*: females have too many eggs for their own nests, therefore resort to parasitism
2. *specialized parasites*: some females engage in a life-long parasitic behaviour
3. *last resort*: parasitic behaviour is resorted to after a female loses her eggs or nest

### Analysis

To begin, we loaded the necessary libraries and read in the data.
The data was downloaded from the [GitHub repository](https://github.com/monogenea/cuckooParasitism) accompanying the tutorial and was saved locally as "data/cuckoo_nesting_data.xlsx".

```{r, eval=FALSE}
# The packages are actually loaded in the 'setup' chunk for the Rmd.
library(rethinking)
library(readxl)
library(tidyverse)
```

```{r, warning=FALSE}
# Load Female Reproductive Output data
fro <- read_excel("data/cuckoo_nesting_data.xlsx", sheet = "Female Reproductive Output") %>%
    as_tibble() %>%
    janitor::clean_names()
```


```{r, echo=FALSE}
fro
```

```{r, echo=FALSE}
skimr::skim(fro)
```


There are a bunch of missing values in the data - in fact, only 57% of the measurements are complete.
The 'rethinking' package has imputation, but 'greta' does not.

### Zero-inflated Poisson regression of fledged egg counts

The `eggs_fledged` variable is a count variable and should be modeled by a Poisson.
However, there are a lot of zeros, therefore, we must use the zero-inflated Poisson regression.

> Zero-inflated poisson regression is used to model count data that has an excess of zero counts.
> Further, theory suggests that the excess zeros are generated by a separate process from the count values and that the excess zeros can be modeled independently.
> Thus, the zip model has two parts, a poisson count model and the logit model for predicting excess zeros.

(["Zero-Inflated Poisson Regression" by the UCLA Institute for Digital Research & Education](https://stats.idre.ucla.edu/r/dae/zip/))


#### Data preparation

We removed rows missing data in the `"eggs_fledged"` column, but the remaining missing values will be imputed.
We then re-coded the `female_id_coded`, `group_id_coded`, and `year`as integers.
These will be groups that we take into account with the model.
Finally, we z-scale the `min_age`, ``group_size`, and `mean_eggsize` to standardize the values, saving the values to new columns with a `z` suffix.

```{r}
fro %>% 
    filter(!is.na(eggs_fledged)) %>%
    mutate(
        female_id = as.integer(factor(female_id_coded)),
        year_id = as.integer(factor(year)),
        group_id = as.integer(factor(group_id_coded)),
        Min_age_Z = scale(min_age),
        Group_size_Z = scale(group_size),
        Mean_eggsize_Z = scale(mean_eggsize)
    )
```

Below is the proposed model for the model of fledged egg counts.
The logit-link restricts the model probabilities to values between 0 and 1.

$$
EggsFledged \sim ZIPoisson(p, \lambda_i) \\
logit(p) = \alpha_p \\
log(\lambda_i) = \alpha + \alpha_{female_i} + \alpha_{year_i} + \alpha_{group_i} + Parasite_i\beta_P  + MinAgeZ_i\beta_A + GroupSizeZ_i\beta_{GS} + MeanEggSizeZ_i\beta_{ES} + Parasite_iMinAgeZ_i\beta_{PA} \\
GroupSizeZ_i, MeanEggSizeZ_i \sim N(0, 3) \\
\alpha_{female_i} ~ N(0, \sigma_1) \\
\alpha_{year_i} ~ N(0, \sigma_2) \\
\alpha_{group_i} ~ N(0, \sigma_3) \\
\sigma_1, \sigma_2, \sigma_3 \sim HalfCauchy(0, 1) \\
\alpha_p, \alpha \sim N(0, 3) \\
\beta_P, \beta_A, \beta_{GS}, \beta_{ES}, \beta_{PA} \sim N(0, 2) \\
$$

Below is the code that actually puts this in action.

**TODO: parse this into smaller chunks with explanations.**

```{r}
eggsFMod <- map2stan(alist(
Eggs_fledged ~ dzipois(p, lambda),
logit(p) <- ap,
log(lambda) <- a + a_fem[female_id] + a_year[year_id] + a_group[group_id] +
Parasite*bP + Min_age_Z*bA + Group_size_Z*bGS + Mean_eggsize_Z*bES +
Parasite*Min_age_Z*bPA,
Group_size_Z ~ dnorm(0, 3),
Mean_eggsize_Z ~ dnorm(0, 3),
a_fem[female_id] ~ dnorm(0, sigma1),
a_year[year_id] ~ dnorm(0, sigma2),
a_group[group_id] ~ dnorm(0, sigma3),
c(sigma1, sigma2, sigma3) ~ dcauchy(0, 1),
c(ap, a) ~ dnorm(0, 3),
c(bP, bA, bGS, bES, bPA) ~ dnorm(0, 2)),
data = fro,
iter = 5e3, warmup = 1e3, chains = 4, cores = 4)
 
# Check posterior dists
precis(eggsFMod, prob = .95) # use depth = 2 for varying intercepts

```

